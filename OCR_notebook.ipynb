{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76d1c87",
   "metadata": {},
   "source": [
    "## 0 Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9d2c1",
   "metadata": {},
   "source": [
    "We first generate images for training using ```trdg``` package. Using the code ```trdg -i words.txt -c 20000 --output_dir data/train```; it's the same for testing\n",
    "\n",
    "Then we create a custom PyTorch dataset object, which has ```__len__``` and ```__getitem__``` methods. We also specify the path to the dataset and apply transform to it. The transforms are grayscaling, normalizing, and converting to tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ae192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dtype = torch.float32 # using float 32 throughout\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c532046",
   "metadata": {},
   "source": [
    "We first define the ```MyCollator()``` function which does the padding horizontally for our images and forming a batch, which is used for ```collate_fn``` in the ```DataLoader``` object in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyCollator(batch):\n",
    "        '''\n",
    "        Parameters:\n",
    "            -batch: list of tuples with dictionary {'img':Image,'idx':Index}, where img is a tensor\n",
    "                    of the shape C * H * W\n",
    "        '''\n",
    "        width = [item['img'].shape[2] for item in batch] # list of widths of images C H W\n",
    "\n",
    "        # print(max(width)) you can check if collate is working properly\n",
    "        \n",
    "        indices = [item['idx'] for item in batch] # list of indices\n",
    "\n",
    "        imgs = torch.ones([len(batch),batch[0]['img'].shape[0],\\\n",
    "            batch[0]['img'].shape[1], max(width)],dtype=dtype) # create array of ones for padding\n",
    "        \n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                # fit image into the array generated, only altering the width\n",
    "                # by replacing ones with the original but keeping extras in new\n",
    "                imgs[idx,:,:,0:width[idx]] = item['img']\n",
    "            except:\n",
    "                print(f'{imgs.shape} \\t ERROR \\n')\n",
    "        # Forming the batch\n",
    "        item = {'img':imgs,'idx':indices}\n",
    "\n",
    "        if 'label' in batch[0].keys():\n",
    "            # if label is present then also make it a list and attach to batch\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['labels'] = labels\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431b6c4",
   "metadata": {},
   "source": [
    "Now we define the dataset with an option of specifying the train or test data path; note how we tell ```__getitem__``` to return a dictionary of image, indices, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDset(Dataset):\n",
    "    '''Create own dataset as subclass of Dataset.'''\n",
    "    def __init__(self,opt={'path':'data','imgdir':'\\train'}):\n",
    "        '''\n",
    "        Initialize MyDset object.\n",
    "        Parameters:\n",
    "            -opt: a dictionary of strings indicating the location/path\n",
    "        '''\n",
    "        super(MyDset,self).__init__()\n",
    "        \n",
    "        self.path = os.path.join(opt['path'],opt['imgdir']) # To the directory storing the images\n",
    "\n",
    "        self.images = os.listdir(self.path) # List the images name\n",
    "\n",
    "        self.nsamp = len(self.images) \n",
    "\n",
    "        f = lambda x: os.path.join(self.path,x) # Join the path and the name\n",
    "        self.imagepaths = list(map(f,self.images))\n",
    "\n",
    "        self.transform = transforms.Compose([\\\n",
    "            transforms.Grayscale(1),\\\n",
    "                transforms.ToTensor(),\\\n",
    "                    transforms.Normalize((0.5,), (0.5,))\\\n",
    "                        ]) # applying transforms to the images\n",
    "\n",
    "        # self.collate_fn = SynthCollator # specified mean to form batches\n",
    "    def __len__(self):\n",
    "        '''Return length of dataset.'''\n",
    "        return self.nsamp\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Return single tensor label pair'''\n",
    "        imagepath = self.imagepaths[index]\n",
    "        name = os.path.basename(imagepath) # return final component of path\n",
    "        img = Image.open(imagepath)\n",
    "\n",
    "        if self.transform is not None: # transform images\n",
    "            img = self.transform(img)\n",
    "\n",
    "        item = {'img': img, 'idx': index} # item to return\n",
    "        item['label'] = name.split('_')[0] # the word in the image # image is named in name_0x0x0x this way\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefebdc",
   "metadata": {},
   "source": [
    "We now create a ```DataLoader``` object from the dataset defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5eb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = MyDset(opt={'path':'data/images','imgdir':'train'})\n",
    "dset_test  = MyDset(opt={'path':'data/images','imgdir':'test'})\n",
    "\n",
    "collate_func = MyCollator\n",
    "\n",
    "NUM_TRAIN =int(len(dset_train)*0.9)\n",
    "\n",
    "# Define the training and its validation dataset\n",
    "loader_train = DataLoader(dset_train,\\\n",
    "    batch_size = 20,\\\n",
    "        collate_fn = collate_func,\\\n",
    "            sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "loader_val = DataLoader(dset_train,\\\n",
    "    batch_size = 20,\\\n",
    "        collate_fn=collate_func,\\\n",
    "            sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, len(dset_train))))\n",
    "\n",
    "loader_test = DataLoader(dset_test,\\\n",
    "    batch_size = 20,\\\n",
    "        collate_fn=collate_func,\\\n",
    "            sampler=sampler.SubsetRandomSampler(range(1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a875fb5",
   "metadata": {},
   "source": [
    "We can try to print out to see if `collate_fn` is working properly, each batch is of size $20$, and the widths of the images are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97278098",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loader_train:\n",
    "    # print(x)\n",
    "    # print(x['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc4996",
   "metadata": {},
   "source": [
    "We try to visualize some of the images. Note the images have not been padded to the same width, as we have not applied the ```collate_fn``` until the ```DataLoader``` stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd74225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(1,3*3+1):\n",
    "    img = dset_train[i]['img'][0,:,:]\n",
    "    print(img.shape)\n",
    "    fig.add_subplot(3,3,i)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b908a",
   "metadata": {},
   "source": [
    "# 1 Implementing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e49ec",
   "metadata": {},
   "source": [
    "We will now implement the model in Pytorch as a Lightning Module.  \n",
    "\n",
    "\n",
    "The network consists of consecutive convolutional layers with max-pooling layers of $2\\times 2$ and $2 \\times 1$; the conv layers produce feature maps as column vectors, which act as the sequential input for the RNN. (Include paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f5c9c",
   "metadata": {},
   "source": [
    "We first deal with the RNN, which is bidirectional and uses the LSTM structure. The code is based on [this repo](https://github.com/meijieru/crnn.pytorch/tree/master/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLR(nn.Module):\n",
    "    '''Implement the bidirectional LSTM RNN.'''\n",
    "    def __init__(self,nIn,nHidden,nOut):\n",
    "        '''\n",
    "        Initialize the BLR.\n",
    "        Parameters:\n",
    "            -nIn: input size\n",
    "            -nHidden: hidden size\n",
    "            -nOut: output size\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(nIn,nHidden,\\\n",
    "            bidirectional=True)\n",
    "        # final FC necessary??\n",
    "        self.linear = nn.Linear(2*nHidden,nOut) # bi-direction\n",
    "\n",
    "    def forward(self,input):\n",
    "        # https://discuss.pytorch.org/t/why-do-we-need-flatten-parameters-when-using-rnn-with-dataparallel/46506\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        r,_ = self.rnn(input) # output of LSTM is out,(hidden,cell)\n",
    "        L, N, H = r.size() # getting sequence len, batch size, and hidden size*2\n",
    "\n",
    "        r_alt = r.view(L*N,H)\n",
    "        output = self.linear(r_alt)\n",
    "\n",
    "        output = output.view(L,N,-1) # out is now a sequence of L each term a vector of size N\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989008f7",
   "metadata": {},
   "source": [
    "We test our implementation of ```BLR```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c638f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBLR():\n",
    "    '''Test output size of BLR.'''\n",
    "    model = BLR(512,16,26)\n",
    "    input = torch.zeros((25,64,512)) # 100*32 image, batch_size=64, 512 final features\n",
    "    output = model(input)\n",
    "    return output.size()\n",
    "\n",
    "testBLR() # Expect L*N*class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13d1da",
   "metadata": {},
   "source": [
    "Now we implement the full CRNN as in the paper. The structure is as follows:  \n",
    "\n",
    "$3 \\times 3 $ ```Conv```, $64$ feature maps  \n",
    "$2 \\times 2 $ ```MaxPool2D```, stride = $2$, img_size halved  \n",
    "\n",
    "$N \\times H \\times W = 64 \\times 16 \\times 64$  \n",
    "\n",
    "$3 \\times 3 $ ```Conv```, $128$ feature maps  \n",
    "$2 \\times 2 $ ```MaxPool2D```, stride = $2$, img_size halved   \n",
    "\n",
    "$N \\times H \\times W = 128 \\times 8 \\times 32$  \n",
    "\n",
    "$3 \\times 3 $ ```Conv```, $256$ feature maps  \n",
    "$3 \\times 3 $ ```Conv```, $256$ feature maps  (can be omitted?)\n",
    "\n",
    "$2 \\times 1 $ ```MaxPool2D```, stride = $2$, img_size halved vertically  \n",
    "\n",
    "$N \\times H \\times W = 256 \\times 4 \\times 16$  \n",
    "\n",
    "$3 \\times 3 $ ```Conv```, $512$ feature maps    \n",
    "\n",
    "$BatchNorm$  Layer.\n",
    "\n",
    "$2 \\times 1 $ ```MaxPool2D```, stride = $2$, img_size halved vertically  \n",
    "\n",
    "$N \\times H \\times W = 512 \\times 2 \\times 16$  \n",
    "\n",
    "$2 \\times 2 $ ```Conv```, $512$ feature maps, img_size halved vertically \n",
    "\n",
    "$N \\times H \\times W = 512 \\times 1 \\times 16$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd82e68",
   "metadata": {},
   "source": [
    "We define a ```Conv``` layer function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myConv(ker_size=3,in_chan=1,out_chan=64,\\\n",
    "    nPad=1, batchNorm=False, leaky=False,label=00):\n",
    "    '''\n",
    "    Implement custom conv layer.\n",
    "    Parameters:\n",
    "        -ker_size: int, size of \n",
    "        -in_chan: int, the input size\n",
    "        -out_chan: int, the output size\n",
    "        -nPad: tuple or \n",
    "        -batchNorm: bool\n",
    "        -maxPool: bool\n",
    "        -label: to distinguish different layers\n",
    "    Output:\n",
    "        -A list of nn objects\n",
    "    '''\n",
    "    out = nn.Sequential()\n",
    "    conv_layer = nn.Conv2d(in_channels=in_chan,out_channels=out_chan,\\\n",
    "        kernel_size=ker_size,padding=nPad,stride=1)\n",
    "        \n",
    "    out.add_module('conv{0}'.format(label),conv_layer)\n",
    "    \n",
    "    if batchNorm:\n",
    "        out.add_module('bn{0}'.format(label),nn.BatchNorm2d(out_chan))\n",
    "    if leaky:\n",
    "        out.add_module('leaky{0}'.format(label),nn.LeakyReLU(0.2, inplace=True))\n",
    "    else:\n",
    "        out.add_module('relu{0}'.format(label),nn.ReLU(inplace=True))\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ba5b7",
   "metadata": {},
   "source": [
    "Now we define the network as a `LightningModule`, including the **CTC** loss function and `forward` for testing. We use some helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ac724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import labelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(pl.LightningModule):\n",
    "    '''\n",
    "    Implement the CRNN.\n",
    "    Parameters:\n",
    "        -opt: a dictionary containing the paramters\n",
    "            -leaky: boolean for ReLU\n",
    "            -nHidden: int for no. of RNN layers\n",
    "            -nClass: int for class of characters\n",
    "        opt = {'leaky':False,'nHidden':256,'nClass':26,'lr':1e-3})\n",
    "    '''\n",
    "    def __init__(self, \\\n",
    "        opt = {'leaky':False,'nHidden':256,'nClass':26, 'lr':1e-3}):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.converter = labelConverter(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        self.lr = opt['lr']\n",
    "\n",
    "        leaky = opt['leaky']\n",
    "        nHidden = opt['nHidden']\n",
    "        nClass = opt['nClass']\n",
    "\n",
    "        group = nn.Sequential()\n",
    "\n",
    "        group.add_module('conv1',myConv(leaky=leaky))\n",
    "        group.add_module('mpool1',nn.MaxPool2d(2,2))\n",
    "\n",
    "        group.add_module('conv2',myConv(3,64,128,1,leaky=leaky))\n",
    "        group.add_module('mpool2',nn.MaxPool2d(2,2))\n",
    "\n",
    "        group.add_module('conv3',myConv(3,128,256,1,leaky=leaky))\n",
    "        group.add_module('mpool3',nn.MaxPool2d((2,2),(2,1),(0,1)))\n",
    "        \n",
    "        # # group.add_module('conv3_5',myConv(3,128,256,1,leaky=leaky))\n",
    "\n",
    "        group.add_module('conv4',myConv(3,256,512,1,leaky=leaky,batchNorm=True))\n",
    "\n",
    "        group.add_module('conv5',myConv(3,512,512,1,leaky=leaky,batchNorm=True))\n",
    "        group.add_module('mpool5',nn.MaxPool2d((2,2),(2,1),(0,1)))\n",
    "\n",
    "        group.add_module('conv_final',myConv(2,512,512,0,leaky=leaky,batchNorm=True)) \n",
    "\n",
    "        # # PyTorch does not support asymmetrical padding\n",
    "\n",
    "        self.cnn = group\n",
    "\n",
    "        self.rnn = nn.Sequential(BLR(512,nHidden,nHidden),\\\n",
    "            BLR(nHidden,nHidden,nClass))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        '''Inference in PyTorch Lightning'''\n",
    "        convolved = self.cnn(input)\n",
    "\n",
    "        N,C,H,W = convolved.size()\n",
    "\n",
    "        assert H==1,'Height should be 1'\n",
    "        # reshaping into a sequence of length Width\n",
    "        # each term is a batch of N vectors of C=512\n",
    "\n",
    "        # height is 1 so squeeze\n",
    "        convolved = convolved.squeeze(2)\n",
    "\n",
    "        # now length of seq = W, batch size N, and features C=512\n",
    "        convolved = convolved.permute(2,0,1) \n",
    "\n",
    "        # Expect L*N*class\n",
    "        output = self.rnn(convolved)\n",
    "\n",
    "        log_probs = F.log_softmax(output, dim = 2)\n",
    "\n",
    "        # Collect probabilities and indices of each image's max class\n",
    "        probs, preds = log_probs.max(2)\n",
    "        preds_size = torch.IntTensor([preds.size(0)] * N)\n",
    "\n",
    "        # Convert back to string\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        output = self.converter.decode(preds, preds_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        '''\n",
    "        Compute and return training loss.\n",
    "            Parameters:\n",
    "            - batch: output of DataLoader\n",
    "            - batch_idx: integer displaying the index of this batch\n",
    "        '''\n",
    "        criterion = nn.CTCLoss() # using CTC-Loss\n",
    "\n",
    "        images = batch['img']\n",
    "        texts = batch['labels']\n",
    "        # Pass through network\n",
    "        convolved = self.cnn(images)\n",
    "        N,C,H,W = convolved.size()\n",
    "        assert H==1,'Height should be 1'\n",
    "        convolved = convolved.squeeze(2)\n",
    "        convolved = convolved.permute(2,0,1)\n",
    "        output = self.rnn(convolved)\n",
    "\n",
    "        # Get log_prob\n",
    "        log_probs = F.log_softmax(output, dim = 2) # along final class dimension \n",
    "        T, N, C = log_probs.size()\n",
    "        input_len = torch.LongTensor([T for i in range(N)])\n",
    "\n",
    "        # Encoding the words\n",
    "        targets, target_len = self.converter.encode(texts)\n",
    "        targets= targets.view(-1).contiguous() # make into contiguous storage in memory\n",
    "\n",
    "        # Compute the loss with CTC\n",
    "        loss = criterion(log_probs, targets, input_len, target_len)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Validation after each epoch'''\n",
    "        images = batch['img']\n",
    "        texts = np.array(batch['labels'])\n",
    "\n",
    "        out = np.array(self.forward(images))\n",
    "        \n",
    "        val_acc = torch.tensor(sum(out==texts), dtype = torch.float32)\n",
    "        \n",
    "        self.log_dict({'val_acc': val_acc})\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''Define optimizer'''\n",
    "        lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = lr)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2017824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch has a different max method for its tensors\n",
    "test_x = torch.rand(3,5,4)\n",
    "vals, idx = test_x.max(2)\n",
    "idx.size(0) # 3 - batch size\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38026e8",
   "metadata": {},
   "source": [
    "We test our implementation of ```CRNN``` for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f745bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCRNN_forward():\n",
    "    '''Test output size of CRNN.'''\n",
    "    opt = {'leaky':False,'nHidden':256,'nClass':26, 'lr':1e-3}\n",
    "    model = CRNN(opt)\n",
    "    input = torch.rand((64,1,32,128)) # 32*128 image, 1 color channel, batch_size=64, 512 final features\n",
    "    output = model(input)\n",
    "    print(output[0])\n",
    "    return len(output)\n",
    "\n",
    "testCRNN_forward() # Expect L*N*class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64df37c",
   "metadata": {},
   "source": [
    "# 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeef444",
   "metadata": {},
   "source": [
    "We now do the training using `Lightning Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN()\n",
    "\n",
    "# trainer = pl.Trainer(check_val_every_n_epoch = 1, gpus = 1, max_epochs = 20)\n",
    "\n",
    "trainer = pl.Trainer(gpus = 1, max_epochs = 20)\n",
    "\n",
    "trainer.fit(model, train_dataloader = loader_train, val_dataloaders = loader_val)\n",
    "\n",
    "# trainer.test(loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da181f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
